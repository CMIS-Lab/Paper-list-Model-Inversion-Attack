# ğŸ›¡ï¸ Model Inversion Attack Papers Collection 

> ğŸ“š **A comprehensive collection of research papers on Model Inversion Attack in machine learning.**  
> ğŸ“ **Maintained by**: Dr. [Shanzhong Lei] | [Chongqing University of Posts and Telecommunications] | [szhonglason@gmail.com]

## ğŸ“– About This Repository

This repository serves as a curated collection of academic papers focusing on **attack methods and defense mechanisms** in machine learning. Our goal is to provide researchers, practitioners, and students with a comprehensive overview of the current state-of-the-art in this critical security domain.

## What is the model inversion attack?

A model inversion attack is a privacy attack where the attacker is able to reconstruct the original samples that were used to train the synthetic model from the generated synthetic data set. (Mostly.ai)

The goal of model inversion attacks is to recreate training data or sensitive attributes.
(Chen et al, 2021.)

In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. (Wang et al, 2021.)


### ğŸ¯ **Repository Purpose:**
- ğŸ† **Quality Focus**: Emphasis on high-impact venues. [CCF-Rankings](https://www.ccf.org.cn/en/About_CCF/Media_Center/) now marked with different colors(![arXiv](https://img.shields.io/badge/CCF_A-dc3545) ![Static Badge](https://img.shields.io/badge/CCF_B-ffc107) ![Static Badge](https://img.shields.io/badge/CCF_C-28a745) ![Static Badge](https://img.shields.io/badge/CCF_None-6c757d))
- ğŸ”„ **Regular Updates**: Continuously updated with latest research developments
- ğŸŒ **Easy Access**: Direct links to papers, code repositories, and supplementary materials

### ğŸ“Š **Each paper includes the following evaluation metrics (out of 5 stars):**
- **ğŸ’¡ Motivation**: How well-motivated and significant is the research problem? â­â­â­â­â­ (5/5)
- **ğŸ”§ Method**: How novel and technically sound is the proposed approach? â­â­â­â­â­ (5/5)

<h2 id="awesome-papers"> ğŸ‘‘ Awesome Papers List </h2>

<h3 id="attacks"> 2025 </h3>

* **[2025.02.23]** **[Diffusion-Driven Universal Model Inversion Attack for Face Recognition](https://openreview.net/forum?id=s56xikpD92)** ![Static Badge](https://img.shields.io/badge/ICLR'24-6c757d) [![GitHub stars](https://img.shields.io/github/stars/vtu81/backdoor-toolbox?style=social)]([https://github.com/yunqing-me/AttackVLM](https://github.com/vtu81/backdoor-toolbox)) 
  * Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin
  * **ğŸ“ Summary**: A novel approach that extracts backdoor functionality from infected models to enable accurate detection of backdoor inputs without requiring clean reference data.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (5/5) - Addresses the critical challenge of backdoor detection in practical scenarios where clean data is unavailable
  * **ğŸ”§ Method**: â­â­â­â­â­ (5/5) - Innovative backdoor functionality extraction technique with strong theoretical foundation and empirical validation
